{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"},{"sourceId":167235579,"sourceType":"kernelVersion"},{"sourceType":"kernelVersion","sourceId":167237126}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport polars as pl\nimport pandas as pd\nfrom data_prep_utilities import *\nfrom dataset_descriptions import dataset_full, dataset_small","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-17T16:41:02.138336Z","iopub.execute_input":"2024-03-17T16:41:02.138843Z","iopub.status.idle":"2024-03-17T16:41:05.421227Z","shell.execute_reply.started":"2024-03-17T16:41:02.138793Z","shell.execute_reply":"2024-03-17T16:41:05.420135Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation Notebook\n\nThis notebook loads the data, performs feature selection and engineering, and joins the tables. The end result is a Train/Val/Test split, to be used for any model training.","metadata":{}},{"cell_type":"markdown","source":"## Data Explanation","metadata":{}},{"cell_type":"markdown","source":"A couple notes on data interpretation:\n\nWhere predictors were transformed, columns describing the transformation have been added with a capital letter suffixing the predictor name\n* P - Transform DPD (Days past due)\n* M - Masking categories\n* A - Transform amount\n* D - Transform date\n* T - Unspecified Transform\n* L - Unspecified Transform\n\nOn depths: depth of a table refers to how many num_group# columns are used to index. Each case_id is only featured once for each unique set of indices, although it may not have a listing for every set. The indexing is not necessarily chronological either; dates where num_group1 == 2 may be earlier than dates where num_group1 == 0. It may be useful to pull summary information for each case_id, e.g. min, max, median, fraction_empty.","metadata":{}},{"cell_type":"code","source":"# for exploration purposes: this gives more information about each feature\ndataPath = \"/kaggle/input/home-credit-credit-risk-model-stability/\"\nfeature_definitions = pl.read_csv(dataPath + \"feature_definitions.csv\")\nprint(feature_definitions.head())","metadata":{"execution":{"iopub.status.busy":"2024-03-17T16:41:10.156932Z","iopub.execute_input":"2024-03-17T16:41:10.157532Z","iopub.status.idle":"2024-03-17T16:41:10.362698Z","shell.execute_reply.started":"2024-03-17T16:41:10.157495Z","shell.execute_reply":"2024-03-17T16:41:10.361706Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"shape: (5, 2)\n┌─────────────────────────┬───────────────────────────────────┐\n│ Variable                ┆ Description                       │\n│ ---                     ┆ ---                               │\n│ str                     ┆ str                               │\n╞═════════════════════════╪═══════════════════════════════════╡\n│ actualdpd_943P          ┆ Days Past Due (DPD) of previous … │\n│ actualdpdtolerance_344P ┆ DPD of client with tolerance.     │\n│ addres_district_368M    ┆ District of the person's address… │\n│ addres_role_871L        ┆ Role of person's address.         │\n│ addres_zip_823M         ┆ Zip code of the address.          │\n└─────────────────────────┴───────────────────────────────────┘\n","output_type":"stream"}]},{"cell_type":"code","source":"# for exploration: investigate a particular df or set of dfs\ndf_info = {\n    \"name\":\"tax_registry_c_1\",\n    \"depth\":2,\n}\ntrain_df, submit_df = load_df(**df_info)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-17T16:41:12.871016Z","iopub.execute_input":"2024-03-17T16:41:12.871466Z","iopub.status.idle":"2024-03-17T16:41:14.163246Z","shell.execute_reply.started":"2024-03-17T16:41:12.871432Z","shell.execute_reply":"2024-03-17T16:41:14.162179Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"shape: (5, 5)\n┌─────────┬───────────────────────┬────────────────┬───────────────────┬─────────────────────────┐\n│ case_id ┆ employername_160M_max ┆ num_group1_max ┆ pmtamount_36A_max ┆ processingdate_168D_max │\n│ ---     ┆ ---                   ┆ ---            ┆ ---               ┆ ---                     │\n│ i64     ┆ str                   ┆ i64            ┆ f64               ┆ str                     │\n╞═════════╪═══════════════════════╪════════════════╪═══════════════════╪═════════════════════════╡\n│ 7515    ┆ 733f5e7b              ┆ 11             ┆ 600.0             ┆ 2018-11-26              │\n│ 1288686 ┆ 3e7e3591              ┆ 5              ┆ 7336.8003         ┆ 2019-02-11              │\n│ 1451892 ┆ 12da981f              ┆ 4              ┆ 3744.6423         ┆ 2019-06-18              │\n│ 1446810 ┆ 596cd835              ┆ 5              ┆ 4726.728          ┆ 2019-07-12              │\n│ 4517    ┆ 2f6e71e3              ┆ 5              ┆ 2717.208          ┆ 2019-02-14              │\n└─────────┴───────────────────────┴────────────────┴───────────────────┴─────────────────────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>case_id</th><th>employername_160M_max</th><th>num_group1_max</th><th>pmtamount_36A_max</th><th>processingdate_168D_max</th></tr><tr><td>i64</td><td>str</td><td>i64</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>7515</td><td>&quot;733f5e7b&quot;</td><td>11</td><td>600.0</td><td>&quot;2018-11-26&quot;</td></tr><tr><td>1288686</td><td>&quot;3e7e3591&quot;</td><td>5</td><td>7336.8003</td><td>&quot;2019-02-11&quot;</td></tr><tr><td>1451892</td><td>&quot;12da981f&quot;</td><td>4</td><td>3744.6423</td><td>&quot;2019-06-18&quot;</td></tr><tr><td>1446810</td><td>&quot;596cd835&quot;</td><td>5</td><td>4726.728</td><td>&quot;2019-07-12&quot;</td></tr><tr><td>4517</td><td>&quot;2f6e71e3&quot;</td><td>5</td><td>2717.208</td><td>&quot;2019-02-14&quot;</td></tr></tbody></table></div>"},"metadata":{}}]},{"cell_type":"code","source":"# create a generator to step through features and their descriptions\ncols=train_df.columns\nif df_info['depth'] > 0:\n    cols = [c[:-4] for c in cols]\npl.Config.set_tbl_width_chars(100)\ndesc = feature_definitions.filter(pl.col('Variable').is_in(cols)).rows()\ndef next_row(desc):\n    for row in desc:\n        print(row[0],\":\")\n        print(row[1])\n        yield\nrow = next_row(desc)\nprint(len(desc))","metadata":{"execution":{"iopub.status.busy":"2024-03-17T16:41:23.471321Z","iopub.execute_input":"2024-03-17T16:41:23.471755Z","iopub.status.idle":"2024-03-17T16:41:23.505236Z","shell.execute_reply.started":"2024-03-17T16:41:23.471724Z","shell.execute_reply":"2024-03-17T16:41:23.504095Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"3\n","output_type":"stream"}]},{"cell_type":"code","source":"next(row)","metadata":{"execution":{"iopub.status.busy":"2024-03-17T16:41:24.377485Z","iopub.execute_input":"2024-03-17T16:41:24.377906Z","iopub.status.idle":"2024-03-17T16:41:24.383813Z","shell.execute_reply.started":"2024-03-17T16:41:24.377872Z","shell.execute_reply":"2024-03-17T16:41:24.382445Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"employername_160M :\nEmployer's name.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"markdown","source":"## Example: Generating splits from dataset descriptions\n\nBelow is a small dataset description; in fact, it describes the same dataset used in the starter notebook.","metadata":{}},{"cell_type":"code","source":"####################################################\n# stores dataset info, arguments for load_df\n#    description: notes to self. Ignored by load functions\n#    name: from the actual name of the file, ignoring extra info (e.g., train/train_{NAME}_1.csv)\n#    features (default all): specify columns to keep (ignore all others)\n#    feature_types (default all): from kept features, select only those ending with these tags\n#    depth (default 0): from kaggle description. If >0, aggregation will be performed\n#    agg_max (default True): if depth>0, return the max for each case_id for each a feature\n#    agg_min (default False): if depth>0, return the min for each case_id for each a feature\n#    agg_median (default False): if depth>0, return the max for each case_id for each a feature\n#####################################################\ndataset_small = {\n    \"base\":{\n        \"description\": \"links case_id to WEEK_NUM and target\",\n        \"name\":\"base\",\n    },\n    \"static_0\":{\n        \"description\":\"contains transaction history for each case_id (late payments, total debt, etc)\",\n        \"name\":\"static_0\",\n        \"feature_types\":[\"A\", \"M\"],\n    },\n    \"static_cb\":{\n        \"description\":\"data from an external cb: demographic data, risk assessment, number of credit checks\",\n        \"name\":\"static_cb\",\n        \"feature_types\":[\"A\", \"M\"],\n    },\n    \"person_1_feats_1\":{\n        \"description\":\" internal demographic information: zip code, marital status, gender etc (all hashed)\",\n        \"name\":\"person_1\",\n        \"features\":[\"mainoccupationinc_384A\", \"incometype_1044T\"],\n        \"depth\":1,\n    },\n    \"person_1_feats_2\":{\n        \"description\":\" internal demographic information: zip code, marital status, gender etc (all hashed)\",\n        \"name\":\"person_1\",\n        \"features\":[\"housetype_905L\"],\n        \"depth\":1,\n    },\n    \"credit_bureau_b_2\":{\n        \"description\":\"historical data from an external source, num and value of overdue payments\",\n        \"name\":\"credit_bureau_b_2\",\n        \"features\":[\"pmts_pmtsoverdue_635A\",\"pmts_dpdvalue_108P\"],\n        \"depth\":2,\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2024-03-16T02:42:58.705872Z","iopub.execute_input":"2024-03-16T02:42:58.706265Z","iopub.status.idle":"2024-03-16T02:42:58.715675Z","shell.execute_reply.started":"2024-03-16T02:42:58.706237Z","shell.execute_reply":"2024-03-16T02:42:58.714309Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"We call load_all_dfs to load the specified datasets from csv, select features, aggregate as indicated, then join all.","metadata":{}},{"cell_type":"code","source":"train_df, submission_df = load_all_dfs(dataset_small)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T02:48:45.163866Z","iopub.execute_input":"2024-03-16T02:48:45.164413Z","iopub.status.idle":"2024-03-16T02:49:07.324247Z","shell.execute_reply.started":"2024-03-16T02:48:45.164381Z","shell.execute_reply":"2024-03-16T02:49:07.323188Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"We will only use submission_df at the end. We save our model's results on this submission_df data for kaggle to evaluate. Train_df is passed to our split function, which returns the splits ready for scaling and training.","metadata":{}},{"cell_type":"code","source":"train_sets, val_sets, test_sets = train_val_test_split(train_df, train_split=0.6)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T02:43:24.756234Z","iopub.execute_input":"2024-03-16T02:43:24.756717Z","iopub.status.idle":"2024-03-16T02:43:33.206751Z","shell.execute_reply.started":"2024-03-16T02:43:24.756675Z","shell.execute_reply":"2024-03-16T02:43:33.205510Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"These sets are lists of pandas dfs of the form \n* base (case_id, WEEK_NUM, target)\n* X (all predictor columns)\n* y (target only)","metadata":{}},{"cell_type":"code","source":"train_sets[1].shape # X_train","metadata":{"execution":{"iopub.status.busy":"2024-03-16T02:43:33.208253Z","iopub.execute_input":"2024-03-16T02:43:33.208673Z","iopub.status.idle":"2024-03-16T02:43:33.216101Z","shell.execute_reply.started":"2024-03-16T02:43:33.208640Z","shell.execute_reply":"2024-03-16T02:43:33.214816Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"(915995, 48)"},"metadata":{}}]},{"cell_type":"markdown","source":"We can now use these splits to train a model. Note that depending on the model, there may still be imputation/scaling/other augmentation necessary.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}