{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"},{"sourceId":167483994,"sourceType":"kernelVersion"},{"sourceId":168043888,"sourceType":"kernelVersion"}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport polars as pl\nimport pandas as pd\n\nfrom data_prep_utilities import *\nfrom dataset_descriptions import dataset_full, dataset_example\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer, IterativeImputer\nimport copy\n\nfrom imblearn.over_sampling import SMOTE \nfrom imblearn.combine import SMOTEENN # alternative: adds undersampling to remove noisy data, but much slower!","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-20T21:53:24.115072Z","iopub.execute_input":"2024-03-20T21:53:24.115488Z","iopub.status.idle":"2024-03-20T21:53:27.700490Z","shell.execute_reply.started":"2024-03-20T21:53:24.115450Z","shell.execute_reply":"2024-03-20T21:53:27.699250Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation Notebook\n\nThis notebook loads the data, performs feature selection and engineering, and joins the tables. The end result is a Train/Val/Test split, to be used for any model training.","metadata":{}},{"cell_type":"markdown","source":"## Data Explanation","metadata":{}},{"cell_type":"markdown","source":"A couple notes on data interpretation:\n\nWhere predictors were transformed, columns describing the transformation have been added with a capital letter suffixing the predictor name\n* P - Transform DPD (Days past due)\n* M - Masking categories\n* A - Transform amount\n* D - Transform date\n* T - Unspecified Transform\n* L - Unspecified Transform\n\nOn depths: depth of a table refers to how many num_group# columns are used to index. Each case_id is only featured once for each unique set of indices, although it may not have a listing for every set. The indexing is not necessarily chronological either; dates where num_group1 == 2 may be earlier than dates where num_group1 == 0. It may be useful to pull summary information for each case_id, e.g. min, max, median, fraction_empty.","metadata":{}},{"cell_type":"code","source":"# for exploration purposes: this gives more information about each feature\ndataPath = \"/kaggle/input/home-credit-credit-risk-model-stability/\"\nfeature_definitions = pl.read_csv(dataPath + \"feature_definitions.csv\")\nprint(feature_definitions.head())","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:53:34.429118Z","iopub.execute_input":"2024-03-20T21:53:34.429506Z","iopub.status.idle":"2024-03-20T21:53:34.614409Z","shell.execute_reply.started":"2024-03-20T21:53:34.429476Z","shell.execute_reply":"2024-03-20T21:53:34.613535Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"shape: (5, 2)\n┌─────────────────────────┬───────────────────────────────────┐\n│ Variable                ┆ Description                       │\n│ ---                     ┆ ---                               │\n│ str                     ┆ str                               │\n╞═════════════════════════╪═══════════════════════════════════╡\n│ actualdpd_943P          ┆ Days Past Due (DPD) of previous … │\n│ actualdpdtolerance_344P ┆ DPD of client with tolerance.     │\n│ addres_district_368M    ┆ District of the person's address… │\n│ addres_role_871L        ┆ Role of person's address.         │\n│ addres_zip_823M         ┆ Zip code of the address.          │\n└─────────────────────────┴───────────────────────────────────┘\n","output_type":"stream"}]},{"cell_type":"code","source":"# for exploration: investigate a particular df or set of dfs\ndf_info = {\n    \"name\":\"tax_registry_c_1\",\n    \"depth\":2,\n    \"feature_types\":[\"A\",\"M\"]\n}\ntrain_df, submit_df = load_df(**df_info)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:53:36.137485Z","iopub.execute_input":"2024-03-20T21:53:36.138422Z","iopub.status.idle":"2024-03-20T21:53:38.271794Z","shell.execute_reply.started":"2024-03-20T21:53:36.138380Z","shell.execute_reply":"2024-03-20T21:53:38.270780Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"shape: (5, 3)\n┌─────────┬───────────────────────┬───────────────────┐\n│ case_id ┆ max_employername_160M ┆ max_pmtamount_36A │\n│ ---     ┆ ---                   ┆ ---               │\n│ i64     ┆ str                   ┆ f64               │\n╞═════════╪═══════════════════════╪═══════════════════╡\n│ 1550762 ┆ 717ea6c3              ┆ 2550.0            │\n│ 691594  ┆ 67b8af22              ┆ 1211.0            │\n│ 1487238 ┆ aaa40643              ┆ 6269.8003         │\n│ 1529617 ┆ 67f075e6              ┆ 7302.4            │\n│ 1561301 ┆ 7daec9f3              ┆ 6800.0            │\n└─────────┴───────────────────────┴───────────────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>case_id</th><th>max_employername_160M</th><th>max_pmtamount_36A</th></tr><tr><td>i64</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>1550762</td><td>&quot;717ea6c3&quot;</td><td>2550.0</td></tr><tr><td>691594</td><td>&quot;67b8af22&quot;</td><td>1211.0</td></tr><tr><td>1487238</td><td>&quot;aaa40643&quot;</td><td>6269.8003</td></tr><tr><td>1529617</td><td>&quot;67f075e6&quot;</td><td>7302.4</td></tr><tr><td>1561301</td><td>&quot;7daec9f3&quot;</td><td>6800.0</td></tr></tbody></table></div>"},"metadata":{}}]},{"cell_type":"code","source":"# create a generator to step through features and their descriptions\ncols=train_df.columns\nif df_info['depth'] > 0:\n    cols = [c[4:] for c in cols]\npl.Config.set_tbl_width_chars(100)\ndesc = feature_definitions.filter(pl.col('Variable').is_in(cols)).rows()\ndef next_row(desc):\n    for row in desc:\n        print(row[0],\":\")\n        print(row[1])\n        yield\nrow = next_row(desc)\nprint(len(desc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"next(row)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"markdown","source":"## Example: Generating splits from dataset descriptions\n\nBelow is a small example dataset description; in fact, it describes the same dataset used in the starter notebook.","metadata":{}},{"cell_type":"code","source":"####################################################\n# stores dataset info, arguments for load_df\n#    description: notes to self. Ignored by load functions\n#    name: from the actual name of the file, ignoring extra info (e.g., train/train_{NAME}_1.csv)\n#    features (default all): specify columns to keep (ignore all others)\n#    feature_types (default all): from kept features, select only those ending with these tags\n#    depth (default 0): from kaggle description. If >0, aggregation will be performed\n#    aggs (default [\"agg_max\"]): which aggregations to use (from agg_max, agg_min, agg_median)\n#####################################################\ndataset_example = {\n    \"base\":{\n        \"description\": \"links case_id to WEEK_NUM and target\",\n        \"name\":\"base\",\n    },\n    \"static_0\":{\n        \"description\":\"contains transaction history for each case_id (late payments, total debt, etc)\",\n        \"name\":\"static_0\",\n        \"feature_types\":[\"A\", \"M\"],\n    },\n    \"static_cb\":{\n        \"description\":\"data from an external cb: demographic data, risk assessment, number of credit checks\",\n        \"name\":\"static_cb\",\n        \"feature_types\":[\"A\", \"M\"],\n    },\n    \"person_1_feats_1\":{\n        \"description\":\" internal demographic information: zip code, marital status, gender etc (all hashed)\",\n        \"name\":\"person_1\",\n        \"features\":[\"mainoccupationinc_384A\", \"incometype_1044T\"],\n        \"depth\":1,\n    },\n    \"person_1_feats_2\":{\n        \"description\":\" internal demographic information: zip code, marital status, gender etc (all hashed)\",\n        \"name\":\"person_1\",\n        \"features\":[\"housetype_905L\"],\n        \"depth\":1,\n    },\n    \"credit_bureau_b_2\":{\n        \"description\":\"historical data from an external source, num and value of overdue payments\",\n        \"name\":\"credit_bureau_b_2\",\n        \"features\":[\"pmts_pmtsoverdue_635A\",\"pmts_dpdvalue_108P\"],\n        \"depth\":2,\n    }\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We call load_all_dfs to load the specified datasets from csv, select features, aggregate as indicated, then join all.","metadata":{}},{"cell_type":"code","source":"train_df, submission_df = load_all_dfs(dataset_example)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:53:47.947527Z","iopub.execute_input":"2024-03-20T21:53:47.947913Z","iopub.status.idle":"2024-03-20T21:54:21.450853Z","shell.execute_reply.started":"2024-03-20T21:53:47.947884Z","shell.execute_reply":"2024-03-20T21:54:21.449967Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"We will only use submission_df at the end. We save our model's results on this submission_df data for kaggle to evaluate. Train_df is passed to our split function, which returns the splits ready for scaling and training. We also pass in the submission_df to standardize format.","metadata":{}},{"cell_type":"code","source":"data = train_val_test_split(train_df, submission_df, train_split=0.6)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:54:21.452307Z","iopub.execute_input":"2024-03-20T21:54:21.452834Z","iopub.status.idle":"2024-03-20T21:54:30.792677Z","shell.execute_reply.started":"2024-03-20T21:54:21.452803Z","shell.execute_reply":"2024-03-20T21:54:30.791287Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"\"data\" has attributes for each split (train, val, test, and submit), with each split containing \n* base (case_id, WEEK_NUM, target)\n* X (all predictor columns)\n* y (target only; not present for submission, of course)","metadata":{}},{"cell_type":"code","source":"data.train.X.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:54:30.794066Z","iopub.execute_input":"2024-03-20T21:54:30.794503Z","iopub.status.idle":"2024-03-20T21:54:30.831844Z","shell.execute_reply.started":"2024-03-20T21:54:30.794461Z","shell.execute_reply":"2024-03-20T21:54:30.830999Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   amtinstpaidbefduel24m_4187115A  annuity_780A  annuitynextmonth_57A  \\\n0                             NaN        1917.6                   0.0   \n1                             NaN        4937.0                   0.0   \n2                             NaN        3600.0                   0.0   \n3                             NaN        3110.8                   0.0   \n4                             NaN        1218.0                   0.0   \n\n   avginstallast24m_3658937A  avglnamtstart24m_4525187A  \\\n0                        NaN                        NaN   \n1                        NaN                        NaN   \n2                        NaN                        NaN   \n3                        NaN                        NaN   \n4                        NaN                        NaN   \n\n   avgoutstandbalancel6m_4187114A  avgpmtlast12m_4525200A  credamount_770A  \\\n0                             NaN                     NaN          30000.0   \n1                             NaN                     NaN          78000.0   \n2                             NaN                     NaN          60000.0   \n3                             NaN                     NaN          20000.0   \n4                             NaN                     NaN          20300.0   \n\n   currdebt_22A  currdebtcredtyperange_828A  ...  maritalst_893M  \\\n0           0.0                         0.0  ...             NaN   \n1           0.0                         0.0  ...             NaN   \n2           0.0                         0.0  ...             NaN   \n3           0.0                         0.0  ...             NaN   \n4           0.0                         0.0  ...             NaN   \n\n   pmtaverage_3A  pmtaverage_4527227A pmtaverage_4955615A pmtssum_45A  \\\n0            NaN                  NaN                 NaN         NaN   \n1            NaN                  NaN                 NaN         NaN   \n2            NaN                  NaN                 NaN         NaN   \n3            NaN                  NaN                 NaN         NaN   \n4            NaN                  NaN                 NaN         NaN   \n\n   max_mainoccupationinc_384A     max_incometype_1044T  max_housetype_905L  \\\n0                     10800.0            SALARIED_GOVT                 NaN   \n1                     14000.0                 EMPLOYED                 NaN   \n2                     64000.0  PRIVATE_SECTOR_EMPLOYEE                 NaN   \n3                     20000.0                 EMPLOYED                 NaN   \n4                     46000.0            SALARIED_GOVT                 NaN   \n\n   max_pmts_pmtsoverdue_635A max_pmts_dpdvalue_108P  \n0                        NaN                    NaN  \n1                        NaN                    NaN  \n2                        NaN                    NaN  \n3                        NaN                    NaN  \n4                        NaN                    NaN  \n\n[5 rows x 53 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>amtinstpaidbefduel24m_4187115A</th>\n      <th>annuity_780A</th>\n      <th>annuitynextmonth_57A</th>\n      <th>avginstallast24m_3658937A</th>\n      <th>avglnamtstart24m_4525187A</th>\n      <th>avgoutstandbalancel6m_4187114A</th>\n      <th>avgpmtlast12m_4525200A</th>\n      <th>credamount_770A</th>\n      <th>currdebt_22A</th>\n      <th>currdebtcredtyperange_828A</th>\n      <th>...</th>\n      <th>maritalst_893M</th>\n      <th>pmtaverage_3A</th>\n      <th>pmtaverage_4527227A</th>\n      <th>pmtaverage_4955615A</th>\n      <th>pmtssum_45A</th>\n      <th>max_mainoccupationinc_384A</th>\n      <th>max_incometype_1044T</th>\n      <th>max_housetype_905L</th>\n      <th>max_pmts_pmtsoverdue_635A</th>\n      <th>max_pmts_dpdvalue_108P</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>1917.6</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>30000.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10800.0</td>\n      <td>SALARIED_GOVT</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>4937.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>78000.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>14000.0</td>\n      <td>EMPLOYED</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>3600.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>60000.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>64000.0</td>\n      <td>PRIVATE_SECTOR_EMPLOYEE</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>3110.8</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>20000.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>20000.0</td>\n      <td>EMPLOYED</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>1218.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>20300.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>46000.0</td>\n      <td>SALARIED_GOVT</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 53 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Feature Engineering\nAs we now have our training data isolated, we can begin feature engineering with the ability to minimize data leakage. \n\n### Missing Data\nTo start, we need to handle any missing data. We will drop features if they are below a certain completeness threshold in our training data. We may also impute missing data in more complete features if our model requires it.","metadata":{}},{"cell_type":"code","source":"# imputer to replace missing values with most frequent per feature\nimp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n\n# the greatest fraction of a feature we will allow to be NaN before we exclude the column\nMAX_MISSING_FEATURE = 0.45\n# if there is a minimum fraction missing, add an indicator column to show imputed values\nMISSING_INDICATOR_THRESHOLD = 0.1\n# the greatest fraction of a data point we will allow to be missing before we exclude it\nMAX_MISSING_INSTANCE = 0.75\n","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:54:56.564038Z","iopub.execute_input":"2024-03-20T21:54:56.564495Z","iopub.status.idle":"2024-03-20T21:54:56.571041Z","shell.execute_reply.started":"2024-03-20T21:54:56.564460Z","shell.execute_reply":"2024-03-20T21:54:56.569885Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# clean X sets\ndata_no_na = handle_missing_data(data,\n                                 max_missing_feature = MAX_MISSING_FEATURE, \n                                 max_missing_instance = MAX_MISSING_INSTANCE,\n                                 missing_indicator_threshold=MISSING_INDICATOR_THRESHOLD,\n                                 imputer=imp,\n                                )","metadata":{"execution":{"iopub.status.busy":"2024-03-20T21:55:02.160515Z","iopub.execute_input":"2024-03-20T21:55:02.160906Z","iopub.status.idle":"2024-03-20T21:55:02.771623Z","shell.execute_reply.started":"2024-03-20T21:55:02.160877Z","shell.execute_reply":"2024-03-20T21:55:02.769987Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# clean X sets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_no_na \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_missing_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_missing_feature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mMAX_MISSING_FEATURE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_missing_instance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mMAX_MISSING_INSTANCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmissing_indicator_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMISSING_INDICATOR_THRESHOLD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mimputer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/kaggle/usr/lib/data_prep_utilities/data_prep_utilities.py:245\u001b[0m, in \u001b[0;36mhandle_missing_data\u001b[0;34m(data, max_missing_feature, max_missing_instance, missing_indicator_threshold, imputer, string_imputer)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandle_missing_data\u001b[39m(data, max_missing_feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, max_missing_instance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, missing_indicator_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, imputer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, string_imputer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# imputers from: (None, sklearn.impute.SimpleImputer, sklearn.impute.IterativeImputer)\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# if no string_imputer provided, imputer should be able to handle both numeric and string\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m     data_no_na \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241m.\u001b[39mdeepcopy(data)\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m split_name, split_data \u001b[38;5;129;01min\u001b[39;00m data_no_na:\n\u001b[1;32m    247\u001b[0m         X \u001b[38;5;241m=\u001b[39m split_data\u001b[38;5;241m.\u001b[39mX\n","\u001b[0;31mNameError\u001b[0m: name 'copy' is not defined"],"ename":"NameError","evalue":"name 'copy' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# removed columns with their fraction missing\nX_train = data.train.X\nX_clean = data_no_na.train.X\n(X_train[list(set(X_train.columns)-set(X_clean.columns))].isna().sum()/len(X_train)).sort_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical data to dummy variables\nNext, convert categorical columns for all to dummy variables:","metadata":{}},{"cell_type":"code","source":"MAX_DUMMIES = 5\n# there will be two additional dummies for each categorical: \"Unknown\" and \"nan\"\n# If there are more categories than our max, the least common items will be compressed into \"Unknown\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we must perform all dummy creation at the same time.\n# if values in X_test are not found in X_train, they need to be marked 'Unknown' even if they are most frequent in X_test\ndata_with_dummies = cat_to_dummies(data_no_na, max_categories=MAX_DUMMIES)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_with_dummies.train.X.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SMOTE for target imbalance\nWe need to fix the dataset imbalance. For this we will use imbalanced-learn's implementation of SMOTE. Note that this implementation requires no NaN values, and cannot handle strings.","metadata":{}},{"cell_type":"code","source":"data_with_dummies.train.y.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oversampler = SMOTE()\nX_train_smote, y_train_smote = oversampler.fit_resample(data_with_dummies.train.X, data_with_dummies.train.y)\ndata_with_dummies.train.X = X_train_smote\ndata_with_dummies.train.y = y_train_smote","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_smote.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline\nInstead of following the above steps, we could instead pass the parameters to data_prep_pipeline:","metadata":{}},{"cell_type":"code","source":"parameters = {\n    \"train_split\"=0.6\n    \"max_missing_feature\" = 0.45, \n    \"max_missing_instance\" = 0.75,\n    \"missing_indicator_threshold\"=0.1,\n    \"imputer\"=SimpleImputer(missing_values=np.nan, strategy='most_frequent'),\n    \"string_imputer\"=None,\n    \"max_categories\"=5,\n    \"oversampler\"=SMOTE()\n}\n\n# # this may take a while...\n# data = data_prep_pipeline(dataset_example, **parameters)\n\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}