{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"},{"sourceId":167235579,"sourceType":"kernelVersion"},{"sourceType":"kernelVersion","sourceId":167237126}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport polars as pl\nimport pandas as pd\nfrom data_prep_utilities import *\nfrom dataset_descriptions import dataset_full, dataset_example","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-19T22:47:00.066842Z","iopub.execute_input":"2024-03-19T22:47:00.067356Z","iopub.status.idle":"2024-03-19T22:47:01.806819Z","shell.execute_reply.started":"2024-03-19T22:47:00.067309Z","shell.execute_reply":"2024-03-19T22:47:01.805244Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation Notebook\n\nThis notebook loads the data, performs feature selection and engineering, and joins the tables. The end result is a Train/Val/Test split, to be used for any model training.","metadata":{}},{"cell_type":"markdown","source":"## Data Explanation","metadata":{}},{"cell_type":"markdown","source":"A couple notes on data interpretation:\n\nWhere predictors were transformed, columns describing the transformation have been added with a capital letter suffixing the predictor name\n* P - Transform DPD (Days past due)\n* M - Masking categories\n* A - Transform amount\n* D - Transform date\n* T - Unspecified Transform\n* L - Unspecified Transform\n\nOn depths: depth of a table refers to how many num_group# columns are used to index. Each case_id is only featured once for each unique set of indices, although it may not have a listing for every set. The indexing is not necessarily chronological either; dates where num_group1 == 2 may be earlier than dates where num_group1 == 0. It may be useful to pull summary information for each case_id, e.g. min, max, median, fraction_empty.","metadata":{}},{"cell_type":"code","source":"# for exploration purposes: this gives more information about each feature\ndataPath = \"/kaggle/input/home-credit-credit-risk-model-stability/\"\nfeature_definitions = pl.read_csv(dataPath + \"feature_definitions.csv\")\nprint(feature_definitions.head())","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:47:04.742157Z","iopub.execute_input":"2024-03-19T22:47:04.742809Z","iopub.status.idle":"2024-03-19T22:47:04.975547Z","shell.execute_reply.started":"2024-03-19T22:47:04.742767Z","shell.execute_reply":"2024-03-19T22:47:04.971648Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"shape: (5, 2)\n┌─────────────────────────┬───────────────────────────────────┐\n│ Variable                ┆ Description                       │\n│ ---                     ┆ ---                               │\n│ str                     ┆ str                               │\n╞═════════════════════════╪═══════════════════════════════════╡\n│ actualdpd_943P          ┆ Days Past Due (DPD) of previous … │\n│ actualdpdtolerance_344P ┆ DPD of client with tolerance.     │\n│ addres_district_368M    ┆ District of the person's address… │\n│ addres_role_871L        ┆ Role of person's address.         │\n│ addres_zip_823M         ┆ Zip code of the address.          │\n└─────────────────────────┴───────────────────────────────────┘\n","output_type":"stream"}]},{"cell_type":"code","source":"# for exploration: investigate a particular df or set of dfs\ndf_info = {\n    \"name\":\"tax_registry_c_1\",\n    \"depth\":2,\n    \"feature_types\":[\"A\",\"M\"]\n}\ntrain_df, submit_df = load_df(**df_info)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:49:17.812487Z","iopub.execute_input":"2024-03-19T22:49:17.813016Z","iopub.status.idle":"2024-03-19T22:49:19.729033Z","shell.execute_reply.started":"2024-03-19T22:49:17.812974Z","shell.execute_reply":"2024-03-19T22:49:19.727975Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"shape: (5, 3)\n┌─────────┬───────────────────────┬───────────────────┐\n│ case_id ┆ max_employername_160M ┆ max_pmtamount_36A │\n│ ---     ┆ ---                   ┆ ---               │\n│ i64     ┆ str                   ┆ f64               │\n╞═════════╪═══════════════════════╪═══════════════════╡\n│ 127446  ┆ ec45a5ff              ┆ 2199.8            │\n│ 687087  ┆ d043a1df              ┆ 4682.59           │\n│ 1476377 ┆ e6f36ea5              ┆ 850.0             │\n│ 1317009 ┆ b0572451              ┆ 4363.2            │\n│ 12273   ┆ 01fa175d              ┆ 700.0             │\n└─────────┴───────────────────────┴───────────────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>case_id</th><th>max_employername_160M</th><th>max_pmtamount_36A</th></tr><tr><td>i64</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>127446</td><td>&quot;ec45a5ff&quot;</td><td>2199.8</td></tr><tr><td>687087</td><td>&quot;d043a1df&quot;</td><td>4682.59</td></tr><tr><td>1476377</td><td>&quot;e6f36ea5&quot;</td><td>850.0</td></tr><tr><td>1317009</td><td>&quot;b0572451&quot;</td><td>4363.2</td></tr><tr><td>12273</td><td>&quot;01fa175d&quot;</td><td>700.0</td></tr></tbody></table></div>"},"metadata":{}}]},{"cell_type":"code","source":"# create a generator to step through features and their descriptions\ncols=train_df.columns\nif df_info['depth'] > 0:\n    cols = [c[4:] for c in cols]\npl.Config.set_tbl_width_chars(100)\ndesc = feature_definitions.filter(pl.col('Variable').is_in(cols)).rows()\ndef next_row(desc):\n    for row in desc:\n        print(row[0],\":\")\n        print(row[1])\n        yield\nrow = next_row(desc)\nprint(len(desc))","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:49:19.731166Z","iopub.execute_input":"2024-03-19T22:49:19.732448Z","iopub.status.idle":"2024-03-19T22:49:19.743445Z","shell.execute_reply.started":"2024-03-19T22:49:19.732405Z","shell.execute_reply":"2024-03-19T22:49:19.742156Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"2\n","output_type":"stream"}]},{"cell_type":"code","source":"next(row)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:47:50.831763Z","iopub.execute_input":"2024-03-19T22:47:50.833161Z","iopub.status.idle":"2024-03-19T22:47:50.839328Z","shell.execute_reply.started":"2024-03-19T22:47:50.833097Z","shell.execute_reply":"2024-03-19T22:47:50.838086Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"employername_160M :\nEmployer's name.\n","output_type":"stream"}]},{"cell_type":"code","source":"# # helper function from contest creator starter notebook\n# def set_table_dtypes(df: pl.DataFrame) -> pl.DataFrame:\n#     # implement here all desired dtypes for tables\n#     # the following is just an example\n#     for col in df.columns:\n#         # last letter of column name will help you determine the type\n#         if col[-1] in (\"P\", \"A\"):\n#             df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n#         elif col[-1] == \"D\":\n#             # for dates, we want both the year as a feature, and the month encoded to capture its cyclic nature\n#             date_split = df[col].str.split(by=\"-\")\n#             df = df.with_columns(\n#                 (2*np.pi*date_split.list.get(1).cast(pl.Float32)/12).sin().alias(col[:-1]+\"X\"),\n#                 (2*np.pi*date_split.list.get(1).cast(pl.Float32)/12).cos().alias(col[:-1]+\"Y\"),\n#                 date_split.list.get(0).cast(pl.Int16).alias(col)\n#             )\n\n#     return df\n\n# # helper function to create a list of formatted aggregations given features and aggregation types\n# def create_agg_list(features, aggs):\n#     agg_features = list(filter(lambda x: x not in ['target','case_id', \"num_group1\", \"num_group2\"], features))\n#     nested_lists = [\n#         [pl.max(f).name.prefix(\"max_\") if \"agg_max\" in aggs else None, \n#           pl.min(f).name.prefix(\"min_\") if \"agg_min\" in aggs else None, \n#           pl.median(f).name.prefix(\"median_\") if \"agg_median\" in aggs else None\n#          ] for f in agg_features]\n#     agg_list = []\n#     for l in nested_lists:\n#         agg_list.extend(l)\n#     agg_list = list(filter(lambda x: x is not None, agg_list))\n#     return agg_list\n\n\n# # this function loads a given dataset, performs aggregation if depth >0, and returns train and test sets\n# def load_df(name, depth=0, features=None, feature_types=None, aggs=[\"agg_max\"], description=None):\n#     # TODO: cater to another agg type: selecting only the first (i.e., group1=0 and group2=0)\n#     dataPath = \"/kaggle/input/home-credit-credit-risk-model-stability/csv_files/\"\n#     if features is not None:\n#         if \"case_id\" not in features:\n#             features = ['case_id']+features\n    \n#     results = []\n#     for split in [\"train\", \"test\"]:\n#         # load file; it may have been partitioned into multiple csvs\n#         filenames = os.listdir(dataPath + f\"{split}\")\n#         matching_filenames = [f for f in filenames if f.startswith(f\"{split}_{name}\")]\n#         # load all partitions\n#         df_list = []\n#         for file in matching_filenames:\n#             df_list.append(pl.read_csv(dataPath+split+\"/\"+file).pipe(set_table_dtypes))\n            \n#         # special case: tax registries need feature names standardized\n#         if name == \"tax_registry\":\n#             name_map ={\"M\":\"employerM\", \"A\":\"taxdeductionA\",\"D\":\"processdateD\", \"X\":\"processdateX\",\"Y\":\"processdateY\"}\n#             for i in range(len(df_list)):\n#                 # standardize their names to allow concat\n#                 standard_cols = [name_map[c[-1]] if c not in ['case_id', \"num_group1\"] else c for c in df_list[i].columns]\n#                 df_list[i].columns = standard_cols\n#                 df_list[i] = df_list[i].select(sorted(standard_cols))\n            \n#         df = pl.concat(df_list, how=\"vertical_relaxed\")\n        \n#         # select the columns specified by features and feature_types\n#         if features is None:\n#             features = df.columns\n#         if split == \"test\":\n#             features= list(filter(lambda x: x != \"target\", features))\n#         if feature_types is not None:\n#             features = ['case_id']+[f for f in features if f[-1] in feature_types]\n#         df = df.select(features)\n        \n#         # if depth > 0, aggregate\n#         if depth > 0:\n#             # determine the aggregations to perform\n#             agg_list = create_agg_list(features, aggs)\n#             # groupby and aggregate\n#             df = df.group_by(\"case_id\").agg(agg_list)\n#         results.append(df)\n        \n#     return results\n\n\n# # given a dict of dataset info, this calls load_df to pull data for each and then joins all.\n# def load_all_dfs(datasets):\n#     train = {}\n#     test = {}\n#     for name in datasets:\n#         train_df, test_df = load_df(**datasets[name])\n#         train[name]=train_df\n#         test[name]=test_df\n    \n#     train_base = train.pop('base')\n#     test_base = test.pop('base')\n    \n#     # join all sets\n#     for dataset in train:\n#         train_base = train_base.join(train[dataset], how='left', on='case_id')\n#         test_base = test_base.join(test[dataset], how='left', on='case_id')\n#     return train_base, test_base\n\n\n\n# # from contest creator starter notebook\n# def from_polars_to_pandas(case_ids: pl.DataFrame, df, is_submit=False) -> pl.DataFrame:\n#     cols_pred = []\n#     for col in df.columns:\n#         if col[-1].isupper() and col[:-1].islower():\n#             cols_pred.append(col)\n#     base_cols = [\"case_id\", \"WEEK_NUM\"] if is_submit else [\"case_id\", \"WEEK_NUM\", \"target\"]\n#     return (\n#         df.filter(pl.col(\"case_id\").is_in(case_ids))[base_cols].to_pandas(),\n#         df.filter(pl.col(\"case_id\").is_in(case_ids))[cols_pred].to_pandas(),\n#         None if is_submit else df.filter(pl.col(\"case_id\").is_in(case_ids))[\"target\"].to_pandas()\n#     )\n\n\n\n# def train_val_test_split(train_df, submit_df, train_split=0.9, val_split=0.5):\n#     # the following code is mostly copied from contest creator starter notebook\n#     # although it has been changed to facilitate functional programming\n#     case_ids = train_df[\"case_id\"].unique().shuffle(seed=1)\n#     case_ids_train, case_ids_test = train_test_split(case_ids, train_size=train_split, random_state=1)\n#     case_ids_val, case_ids_test = train_test_split(case_ids_test, train_size=val_split, random_state=1)\n#     case_ids_submit = submit_df[\"case_id\"].unique()\n    \n#     base_train, X_train, y_train = from_polars_to_pandas(case_ids_train, train_df)\n#     base_val, X_val, y_val = from_polars_to_pandas(case_ids_val, train_df)\n#     base_test, X_test, y_test = from_polars_to_pandas(case_ids_test, train_df)\n#     base_submit, X_submit, y_submit = from_polars_to_pandas(case_ids_submit, submit_df, is_submit=True)\n    \n#     for df in [X_train, X_val, X_test, X_submit]:\n#         df = convert_strings(df)\n    \n#     return (\n#         (base_train, X_train, y_train), \n#         (base_val, X_val, y_val), \n#         (base_test, X_test, y_test),\n#         (base_submit, X_submit, y_submit)\n#     )\n\n\n# def cat_to_dummies(X_train, other_dfs = [], max_categories=5):\n#     # select categorical columns\n#     cols_cat = X_train.select_dtypes(include=\"category\").columns\n#     cols_non_cat = X_train.select_dtypes(exclude=\"category\").columns\n#     X_train_cat = X_train[cols_cat]\n        \n#     # condense least common categories to \"Unknown\"\n#     top_n = {}\n#     for col in cols_cat:\n#         categories = X_train_cat[col].dtype.categories\n#         if len(categories) > max_categories:\n#             # find most common in train set\n#             top_n[col] = X_train_cat[col].value_counts().index[:max_categories]\n#             X_train_cat.loc[:,col] = X_train_cat.loc[:,col].apply(lambda x: x if x in top_n[col] else \"Unknown\").astype(X_train_cat.loc[:,col].dtype)\n#         else:\n#             top_n[col] = categories\n    \n#     # create dummies\n#     X_train_dummies = pd.get_dummies(X_train_cat.astype(\"object\"), dummy_na=True)\n    \n#     # join to non categorical\n#     X_train_non_cat = X_train[cols_non_cat]\n#     X_train_combined = pd.concat([X_train_dummies, X_train_non_cat], axis=1)\n    \n    \n#     # repeat for other dfs, using the same categories\n#     other_combined = []\n#     if other_dfs:\n#         for df in other_dfs:\n#             df_cat = df[cols_cat]\n            \n#             # reduce categories using top_n from X_train\n#             for col in cols_cat:\n#                 df_cat.loc[:,col] = df_cat.loc[:,col].apply(lambda x: x if x in top_n[col] else \"Unknown\").astype(df_cat.loc[:,col].dtype)\n                \n#             # create dummies\n#             df_dummies = pd.get_dummies(df_cat.astype(\"object\"), dummy_na=True)\n            \n#             # join to non categorical\n#             df_non_cat = df[cols_non_cat]\n#             other_combined.append(pd.concat([df_dummies, df_non_cat], axis=1))\n                \n    \n#     return X_train_combined, *other_combined","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:47:57.210340Z","iopub.execute_input":"2024-03-19T22:47:57.210779Z","iopub.status.idle":"2024-03-19T22:47:57.226300Z","shell.execute_reply.started":"2024-03-19T22:47:57.210749Z","shell.execute_reply":"2024-03-19T22:47:57.224779Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"markdown","source":"## Example: Generating splits from dataset descriptions\n\nBelow is a small dataset description; in fact, it describes the same dataset used in the starter notebook.","metadata":{}},{"cell_type":"code","source":"####################################################\n# stores dataset info, arguments for load_df\n#    description: notes to self. Ignored by load functions\n#    name: from the actual name of the file, ignoring extra info (e.g., train/train_{NAME}_1.csv)\n#    features (default all): specify columns to keep (ignore all others)\n#    feature_types (default all): from kept features, select only those ending with these tags\n#    depth (default 0): from kaggle description. If >0, aggregation will be performed\n#    aggs (default [\"agg_max\"]): which aggregations to use (from agg_max, agg_min, agg_median)\n#####################################################\n# dataset_example = {\n#     \"base\":{\n#         \"description\": \"links case_id to WEEK_NUM and target\",\n#         \"name\":\"base\",\n#     },\n#     \"static_0\":{\n#         \"description\":\"contains transaction history for each case_id (late payments, total debt, etc)\",\n#         \"name\":\"static_0\",\n#         \"feature_types\":[\"A\", \"M\"],\n#     },\n#     \"static_cb\":{\n#         \"description\":\"data from an external cb: demographic data, risk assessment, number of credit checks\",\n#         \"name\":\"static_cb\",\n#         \"feature_types\":[\"A\", \"M\"],\n#     },\n#     \"person_1_feats_1\":{\n#         \"description\":\" internal demographic information: zip code, marital status, gender etc (all hashed)\",\n#         \"name\":\"person_1\",\n#         \"features\":[\"mainoccupationinc_384A\", \"incometype_1044T\"],\n#         \"depth\":1,\n#     },\n#     \"person_1_feats_2\":{\n#         \"description\":\" internal demographic information: zip code, marital status, gender etc (all hashed)\",\n#         \"name\":\"person_1\",\n#         \"features\":[\"housetype_905L\"],\n#         \"depth\":1,\n#     },\n#     \"credit_bureau_b_2\":{\n#         \"description\":\"historical data from an external source, num and value of overdue payments\",\n#         \"name\":\"credit_bureau_b_2\",\n#         \"features\":[\"pmts_pmtsoverdue_635A\",\"pmts_dpdvalue_108P\"],\n#         \"depth\":2,\n#     }\n# }","metadata":{"execution":{"iopub.status.busy":"2024-03-19T21:05:37.540542Z","iopub.execute_input":"2024-03-19T21:05:37.540971Z","iopub.status.idle":"2024-03-19T21:05:37.553006Z","shell.execute_reply.started":"2024-03-19T21:05:37.540940Z","shell.execute_reply":"2024-03-19T21:05:37.550980Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"We call load_all_dfs to load the specified datasets from csv, select features, aggregate as indicated, then join all.","metadata":{}},{"cell_type":"code","source":"train_df, submission_df = load_all_dfs(dataset_example)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:49:34.930742Z","iopub.execute_input":"2024-03-19T22:49:34.932123Z","iopub.status.idle":"2024-03-19T22:50:12.881759Z","shell.execute_reply.started":"2024-03-19T22:49:34.932074Z","shell.execute_reply":"2024-03-19T22:50:12.880255Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"We will only use submission_df at the end. We save our model's results on this submission_df data for kaggle to evaluate. Train_df is passed to our split function, which returns the splits ready for scaling and training. We also pass in the submission_df to standardize format.","metadata":{}},{"cell_type":"code","source":"train_sets, val_sets, test_sets, submit_sets = train_val_test_split(train_df, submission_df, train_split=0.6)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:50:18.606470Z","iopub.execute_input":"2024-03-19T22:50:18.606963Z","iopub.status.idle":"2024-03-19T22:50:28.552640Z","shell.execute_reply.started":"2024-03-19T22:50:18.606911Z","shell.execute_reply":"2024-03-19T22:50:28.551384Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"These sets are lists of pandas dfs of the form \n* base (case_id, WEEK_NUM, target)\n* X (all predictor columns)\n* y (target only)","metadata":{}},{"cell_type":"code","source":"train_sets[1].shape # X_train","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:50:32.684697Z","iopub.execute_input":"2024-03-19T22:50:32.685202Z","iopub.status.idle":"2024-03-19T22:50:32.692784Z","shell.execute_reply.started":"2024-03-19T22:50:32.685165Z","shell.execute_reply":"2024-03-19T22:50:32.691477Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(915995, 53)"},"metadata":{}}]},{"cell_type":"code","source":"train_sets[1].head()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:50:46.121625Z","iopub.execute_input":"2024-03-19T22:50:46.122319Z","iopub.status.idle":"2024-03-19T22:50:46.167784Z","shell.execute_reply.started":"2024-03-19T22:50:46.122267Z","shell.execute_reply":"2024-03-19T22:50:46.166373Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"   amtinstpaidbefduel24m_4187115A  annuity_780A  annuitynextmonth_57A  \\\n0                             NaN        1917.6                   0.0   \n1                             NaN        4937.0                   0.0   \n2                             NaN        3600.0                   0.0   \n3                             NaN        3110.8                   0.0   \n4                             NaN        1218.0                   0.0   \n\n   avginstallast24m_3658937A  avglnamtstart24m_4525187A  \\\n0                        NaN                        NaN   \n1                        NaN                        NaN   \n2                        NaN                        NaN   \n3                        NaN                        NaN   \n4                        NaN                        NaN   \n\n   avgoutstandbalancel6m_4187114A  avgpmtlast12m_4525200A  credamount_770A  \\\n0                             NaN                     NaN          30000.0   \n1                             NaN                     NaN          78000.0   \n2                             NaN                     NaN          60000.0   \n3                             NaN                     NaN          20000.0   \n4                             NaN                     NaN          20300.0   \n\n   currdebt_22A  currdebtcredtyperange_828A  ...  maritalst_893M  \\\n0           0.0                         0.0  ...             NaN   \n1           0.0                         0.0  ...             NaN   \n2           0.0                         0.0  ...             NaN   \n3           0.0                         0.0  ...             NaN   \n4           0.0                         0.0  ...             NaN   \n\n   pmtaverage_3A  pmtaverage_4527227A pmtaverage_4955615A pmtssum_45A  \\\n0            NaN                  NaN                 NaN         NaN   \n1            NaN                  NaN                 NaN         NaN   \n2            NaN                  NaN                 NaN         NaN   \n3            NaN                  NaN                 NaN         NaN   \n4            NaN                  NaN                 NaN         NaN   \n\n   max_mainoccupationinc_384A     max_incometype_1044T  max_housetype_905L  \\\n0                     10800.0            SALARIED_GOVT                 NaN   \n1                     14000.0                 EMPLOYED                 NaN   \n2                     64000.0  PRIVATE_SECTOR_EMPLOYEE                 NaN   \n3                     20000.0                 EMPLOYED                 NaN   \n4                     46000.0            SALARIED_GOVT                 NaN   \n\n   max_pmts_pmtsoverdue_635A max_pmts_dpdvalue_108P  \n0                        NaN                    NaN  \n1                        NaN                    NaN  \n2                        NaN                    NaN  \n3                        NaN                    NaN  \n4                        NaN                    NaN  \n\n[5 rows x 53 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>amtinstpaidbefduel24m_4187115A</th>\n      <th>annuity_780A</th>\n      <th>annuitynextmonth_57A</th>\n      <th>avginstallast24m_3658937A</th>\n      <th>avglnamtstart24m_4525187A</th>\n      <th>avgoutstandbalancel6m_4187114A</th>\n      <th>avgpmtlast12m_4525200A</th>\n      <th>credamount_770A</th>\n      <th>currdebt_22A</th>\n      <th>currdebtcredtyperange_828A</th>\n      <th>...</th>\n      <th>maritalst_893M</th>\n      <th>pmtaverage_3A</th>\n      <th>pmtaverage_4527227A</th>\n      <th>pmtaverage_4955615A</th>\n      <th>pmtssum_45A</th>\n      <th>max_mainoccupationinc_384A</th>\n      <th>max_incometype_1044T</th>\n      <th>max_housetype_905L</th>\n      <th>max_pmts_pmtsoverdue_635A</th>\n      <th>max_pmts_dpdvalue_108P</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>1917.6</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>30000.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10800.0</td>\n      <td>SALARIED_GOVT</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>4937.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>78000.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>14000.0</td>\n      <td>EMPLOYED</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>3600.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>60000.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>64000.0</td>\n      <td>PRIVATE_SECTOR_EMPLOYEE</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>3110.8</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>20000.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>20000.0</td>\n      <td>EMPLOYED</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>1218.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>20300.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>46000.0</td>\n      <td>SALARIED_GOVT</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 53 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"We can now use these splits to train a model. Note that depending on the model, there may still be imputation/scaling/other augmentation necessary.\n\nNext, convert categorical columns for all to dummy variables:","metadata":{}},{"cell_type":"code","source":"max_categories = 5 \n# there will be two additional dummies for each categorical: \"Unknown\" and \"nan\"\n# If there are more categories than our max, the least common items will be compressed into \"Unknown\"\n\n# we must perform all dummy creation at the same time.\n# if values in X_test are not found in X_train, they need to be marked 'Unknown' even if they are most frequent in X_test\nX_train, X_val, X_test, X_submit = cat_to_dummies(train_sets[1], \n                                                  other_dfs = [val_sets[1], test_sets[1], submit_sets[1]], \n                                                  max_categories=max_categories)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:51:03.959744Z","iopub.execute_input":"2024-03-19T22:51:03.960248Z","iopub.status.idle":"2024-03-19T22:51:12.619100Z","shell.execute_reply.started":"2024-03-19T22:51:03.960205Z","shell.execute_reply":"2024-03-19T22:51:12.617665Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"X_val.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T22:51:14.704142Z","iopub.execute_input":"2024-03-19T22:51:14.704668Z","iopub.status.idle":"2024-03-19T22:51:14.739129Z","shell.execute_reply.started":"2024-03-19T22:51:14.704628Z","shell.execute_reply":"2024-03-19T22:51:14.737657Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"   lastapprcommoditycat_1041M_P109_133_183  \\\n0                                    False   \n1                                    False   \n2                                    False   \n3                                    False   \n4                                    False   \n\n   lastapprcommoditycat_1041M_P12_6_178  \\\n0                                 False   \n1                                 False   \n2                                 False   \n3                                 False   \n4                                 False   \n\n   lastapprcommoditycat_1041M_P148_110_5  \\\n0                                  False   \n1                                  False   \n2                                  False   \n3                                  False   \n4                                  False   \n\n   lastapprcommoditycat_1041M_P159_130_59  lastapprcommoditycat_1041M_Unknown  \\\n0                                   False                               False   \n1                                   False                               False   \n2                                   False                               False   \n3                                   False                               False   \n4                                   False                               False   \n\n   lastapprcommoditycat_1041M_a55475b1  lastapprcommoditycat_1041M_nan  \\\n0                                 True                           False   \n1                                 True                           False   \n2                                 True                           False   \n3                                 True                           False   \n4                                 True                           False   \n\n   lastapprcommoditytypec_5251766M_P111_89_135  \\\n0                                        False   \n1                                        False   \n2                                        False   \n3                                        False   \n4                                        False   \n\n   lastapprcommoditytypec_5251766M_P142_50_170  \\\n0                                        False   \n1                                        False   \n2                                        False   \n3                                        False   \n4                                        False   \n\n   lastapprcommoditytypec_5251766M_P174_113_42  ...  totaldebt_9A  \\\n0                                        False  ...           0.0   \n1                                        False  ...           0.0   \n2                                        False  ...           0.0   \n3                                        False  ...           0.0   \n4                                        False  ...           0.0   \n\n   totalsettled_863A  totinstallast1m_4525188A  pmtaverage_3A  \\\n0                0.0                       NaN            NaN   \n1                0.0                       NaN            NaN   \n2                0.0                       NaN            NaN   \n3                0.0                       NaN            NaN   \n4                0.0                       NaN            NaN   \n\n   pmtaverage_4527227A  pmtaverage_4955615A  pmtssum_45A  \\\n0                  NaN                  NaN          NaN   \n1                  NaN                  NaN          NaN   \n2                  NaN                  NaN          NaN   \n3                  NaN                  NaN          NaN   \n4                  NaN                  NaN          NaN   \n\n   max_mainoccupationinc_384A  max_pmts_pmtsoverdue_635A  \\\n0                     10000.0                        NaN   \n1                     10000.0                        NaN   \n2                    100000.0                        NaN   \n3                     50000.0                        NaN   \n4                     64000.0                        NaN   \n\n   max_pmts_dpdvalue_108P  \n0                     NaN  \n1                     NaN  \n2                     NaN  \n3                     NaN  \n4                     NaN  \n\n[5 rows x 137 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lastapprcommoditycat_1041M_P109_133_183</th>\n      <th>lastapprcommoditycat_1041M_P12_6_178</th>\n      <th>lastapprcommoditycat_1041M_P148_110_5</th>\n      <th>lastapprcommoditycat_1041M_P159_130_59</th>\n      <th>lastapprcommoditycat_1041M_Unknown</th>\n      <th>lastapprcommoditycat_1041M_a55475b1</th>\n      <th>lastapprcommoditycat_1041M_nan</th>\n      <th>lastapprcommoditytypec_5251766M_P111_89_135</th>\n      <th>lastapprcommoditytypec_5251766M_P142_50_170</th>\n      <th>lastapprcommoditytypec_5251766M_P174_113_42</th>\n      <th>...</th>\n      <th>totaldebt_9A</th>\n      <th>totalsettled_863A</th>\n      <th>totinstallast1m_4525188A</th>\n      <th>pmtaverage_3A</th>\n      <th>pmtaverage_4527227A</th>\n      <th>pmtaverage_4955615A</th>\n      <th>pmtssum_45A</th>\n      <th>max_mainoccupationinc_384A</th>\n      <th>max_pmts_pmtsoverdue_635A</th>\n      <th>max_pmts_dpdvalue_108P</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10000.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10000.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>100000.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>50000.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>64000.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 137 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}